{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üèóÔ∏è Atelier Data Lake - Pipeline Bronze ‚Üí Silver ‚Üí Gold\n",
        "\n",
        "Ce notebook vous guide √† travers l'architecture Medallion du Data Lake.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('..')\n",
        "\n",
        "from src.spark_session import get_spark_session\n",
        "from config.settings import SOURCES, BRONZE_PATH, SILVER_PATH, GOLD_PATH\n",
        "from pyspark.sql.functions import lit, current_timestamp, explode, col, trim, upper, lower, regexp_replace, coalesce, when, to_date\n",
        "\n",
        "spark = get_spark_session(\"DataLake_Pipeline\")\n",
        "print(\"‚úÖ Session Spark cr√©√©e!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ü•â √âtape 1: Ingestion vers Bronze\n",
        "\n",
        "La couche Bronze contient les donn√©es brutes, telles qu'elles arrivent des sources.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ingestion Clients CSV ‚Üí Bronze\n",
        "clients_df = (\n",
        "    spark.read\n",
        "    .option(\"header\", True)\n",
        "    .option(\"inferSchema\", True)\n",
        "    .csv(str(SOURCES[\"csv\"] / \"clients.csv\"))\n",
        "    .withColumn(\"_source_file\", lit(\"clients.csv\"))\n",
        "    .withColumn(\"_ingestion_timestamp\", current_timestamp())\n",
        "    .withColumn(\"_source_type\", lit(\"csv\"))\n",
        ")\n",
        "\n",
        "clients_df.write.mode(\"overwrite\").parquet(str(BRONZE_PATH / \"clients\"))\n",
        "print(f\"‚úÖ Clients ing√©r√©s: {clients_df.count()} enregistrements\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ingestion Produits CSV ‚Üí Bronze\n",
        "produits_df = (\n",
        "    spark.read\n",
        "    .option(\"header\", True)\n",
        "    .option(\"inferSchema\", True)\n",
        "    .csv(str(SOURCES[\"csv\"] / \"produits.csv\"))\n",
        "    .withColumn(\"_source_file\", lit(\"produits.csv\"))\n",
        "    .withColumn(\"_ingestion_timestamp\", current_timestamp())\n",
        "    .withColumn(\"_source_type\", lit(\"csv\"))\n",
        ")\n",
        "\n",
        "produits_df.write.mode(\"overwrite\").parquet(str(BRONZE_PATH / \"produits\"))\n",
        "print(f\"‚úÖ Produits ing√©r√©s: {produits_df.count()} enregistrements\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ingestion Commandes JSON ‚Üí Bronze\n",
        "commandes_raw = spark.read.option(\"multiline\", True).json(str(SOURCES[\"json\"] / \"commandes.json\"))\n",
        "commandes_df = (\n",
        "    commandes_raw\n",
        "    .select(explode(\"commandes\").alias(\"c\"))\n",
        "    .select(\"c.*\")\n",
        "    .withColumn(\"_source_file\", lit(\"commandes.json\"))\n",
        "    .withColumn(\"_ingestion_timestamp\", current_timestamp())\n",
        "    .withColumn(\"_source_type\", lit(\"json\"))\n",
        ")\n",
        "\n",
        "commandes_df.write.mode(\"overwrite\").parquet(str(BRONZE_PATH / \"commandes\"))\n",
        "print(f\"‚úÖ Commandes ing√©r√©es: {commandes_df.count()} enregistrements\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ü•à √âtape 2: Transformation Bronze ‚Üí Silver\n",
        "\n",
        "La couche Silver contient les donn√©es nettoy√©es et standardis√©es.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Transformation Clients Bronze ‚Üí Silver\n",
        "bronze_clients = spark.read.parquet(str(BRONZE_PATH / \"clients\"))\n",
        "\n",
        "silver_clients = (\n",
        "    bronze_clients\n",
        "    .withColumn(\"nom\", trim(upper(col(\"nom\"))))\n",
        "    .withColumn(\"prenom\", trim(upper(col(\"prenom\"))))\n",
        "    .withColumn(\"email\", trim(lower(col(\"email\"))))\n",
        "    .withColumn(\"telephone\", regexp_replace(col(\"telephone\"), \"[^0-9]\", \"\"))\n",
        "    .withColumn(\"ville\", coalesce(col(\"ville\"), lit(\"INCONNU\")))\n",
        "    .dropDuplicates([\"email\"])\n",
        "    .withColumn(\"_silver_timestamp\", current_timestamp())\n",
        ")\n",
        "\n",
        "silver_clients.write.mode(\"overwrite\").parquet(str(SILVER_PATH / \"clients\"))\n",
        "print(f\"‚úÖ Clients Silver: {silver_clients.count()} enregistrements\")\n",
        "silver_clients.show(3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Transformation Produits Bronze ‚Üí Silver\n",
        "bronze_produits = spark.read.parquet(str(BRONZE_PATH / \"produits\"))\n",
        "\n",
        "silver_produits = (\n",
        "    bronze_produits\n",
        "    .withColumn(\"nom_produit\", trim(col(\"nom_produit\")))\n",
        "    .withColumn(\"categorie\", trim(upper(col(\"categorie\"))))\n",
        "    .withColumn(\"prix\", when(col(\"prix\") < 0, 0).otherwise(col(\"prix\")))\n",
        "    .withColumn(\"stock\", when(col(\"stock\") < 0, 0).otherwise(col(\"stock\")))\n",
        "    .dropDuplicates([\"produit_id\"])\n",
        "    .withColumn(\"_silver_timestamp\", current_timestamp())\n",
        ")\n",
        "\n",
        "silver_produits.write.mode(\"overwrite\").parquet(str(SILVER_PATH / \"produits\"))\n",
        "print(f\"‚úÖ Produits Silver: {silver_produits.count()} enregistrements\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Transformation Commandes Bronze ‚Üí Silver\n",
        "bronze_commandes = spark.read.parquet(str(BRONZE_PATH / \"commandes\"))\n",
        "\n",
        "silver_commandes = (\n",
        "    bronze_commandes\n",
        "    .withColumn(\"date_commande\", to_date(col(\"date_commande\")))\n",
        "    .withColumn(\"montant_total\", when(col(\"montant_total\") < 0, 0).otherwise(col(\"montant_total\")))\n",
        "    .withColumn(\"statut\", upper(trim(col(\"statut\"))))\n",
        "    .dropDuplicates([\"commande_id\"])\n",
        "    .withColumn(\"_silver_timestamp\", current_timestamp())\n",
        ")\n",
        "\n",
        "silver_commandes.write.mode(\"overwrite\").parquet(str(SILVER_PATH / \"commandes\"))\n",
        "print(f\"‚úÖ Commandes Silver: {silver_commandes.count()} enregistrements\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ü•á √âtape 3: Agr√©gation Silver ‚Üí Gold\n",
        "\n",
        "La couche Gold contient les donn√©es agr√©g√©es pr√™tes pour l'analyse business.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import sum, avg, count, max, min, round as spark_round, year, month\n",
        "\n",
        "# Gold: Ventes par Client\n",
        "silver_clients = spark.read.parquet(str(SILVER_PATH / \"clients\"))\n",
        "silver_commandes = spark.read.parquet(str(SILVER_PATH / \"commandes\"))\n",
        "\n",
        "ventes_par_client = (\n",
        "    silver_commandes\n",
        "    .groupBy(\"client_id\")\n",
        "    .agg(\n",
        "        count(\"commande_id\").alias(\"nombre_commandes\"),\n",
        "        sum(\"montant_total\").alias(\"total_achats\"),\n",
        "        avg(\"montant_total\").alias(\"panier_moyen\"),\n",
        "        max(\"date_commande\").alias(\"derniere_commande\"),\n",
        "        min(\"date_commande\").alias(\"premiere_commande\"),\n",
        "    )\n",
        ")\n",
        "\n",
        "gold_ventes_client = (\n",
        "    silver_clients\n",
        "    .join(ventes_par_client, \"client_id\", \"left\")\n",
        "    .select(\n",
        "        \"client_id\", \"nom\", \"prenom\", \"email\", \"ville\",\n",
        "        \"nombre_commandes\", \"total_achats\",\n",
        "        spark_round(\"panier_moyen\", 2).alias(\"panier_moyen\"),\n",
        "        \"premiere_commande\", \"derniere_commande\",\n",
        "    )\n",
        "    .withColumn(\"_gold_timestamp\", current_timestamp())\n",
        ")\n",
        "\n",
        "gold_ventes_client.write.mode(\"overwrite\").parquet(str(GOLD_PATH / \"ventes_par_client\"))\n",
        "print(f\"‚úÖ Gold Ventes par Client: {gold_ventes_client.count()} enregistrements\")\n",
        "gold_ventes_client.orderBy(col(\"total_achats\").desc()).show(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Gold: KPIs Mensuels\n",
        "gold_kpi = (\n",
        "    silver_commandes\n",
        "    .withColumn(\"annee\", year(\"date_commande\"))\n",
        "    .withColumn(\"mois\", month(\"date_commande\"))\n",
        "    .groupBy(\"annee\", \"mois\")\n",
        "    .agg(\n",
        "        count(\"commande_id\").alias(\"nombre_commandes\"),\n",
        "        sum(\"montant_total\").alias(\"chiffre_affaires\"),\n",
        "        avg(\"montant_total\").alias(\"panier_moyen\"),\n",
        "        sum(\"quantite\").alias(\"articles_vendus\"),\n",
        "    )\n",
        "    .withColumn(\"chiffre_affaires\", spark_round(\"chiffre_affaires\", 2))\n",
        "    .withColumn(\"panier_moyen\", spark_round(\"panier_moyen\", 2))\n",
        "    .orderBy(\"annee\", \"mois\")\n",
        "    .withColumn(\"_gold_timestamp\", current_timestamp())\n",
        ")\n",
        "\n",
        "gold_kpi.write.mode(\"overwrite\").parquet(str(GOLD_PATH / \"kpi_mensuel\"))\n",
        "print(\"‚úÖ Gold KPIs Mensuels cr√©√©s!\")\n",
        "gold_kpi.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fermeture de la session\n",
        "spark.stop()\n",
        "print(\"‚úÖ Pipeline termin√©! Session Spark ferm√©e.\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
